{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from frame_postprocess import *\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### THIS SCRIPT COLLECTS THE RESPONSE FOR MULTIPLE SETS OF GROUND MOTIONS SCALED TO PARTICULAR INTENSITIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Bldg and frame direction #####\n",
    "bldg_name = 'Example_Bldg1'\n",
    "####################################\n",
    "\n",
    "# Data collection inputs\n",
    "minrdrift = 5e-4 # Minimum residual drift to consider\n",
    "fracElement = True # Boolean to collect or not fracture element results ###########CHANGE TO COLLECT FRACTURES###########\n",
    "splice_frac_strain = 2*60/29000 # strain limit to judge that fracture occured in the splice\n",
    "rdrift_out = 'all' # 'max' = only collect the peak RID\n",
    "                   # 'all' = collects RID for every story\n",
    "drift_out = 'abs' # 'abs' = multiple columns with the peak absolute value for each floor\n",
    "                   # 'both' = multiple columns with the peak positive an negative value for each floor\n",
    "\n",
    "# Folder paths\n",
    "results_folder = posixpath.join('1_Raw_NLRHA_results')\n",
    "save_folder = posixpath.join('2_Collected_NLRHA_results')\n",
    "\n",
    "# load cases inputs to locate data to collect\n",
    "input_filepath = posixpath.join(results_folder, 'building_info_for_plot.csv')\n",
    "inputs = pd.read_csv(input_filepath)\n",
    "\n",
    "# Get variables\n",
    "n_cases = len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OBJECTID</th>\n",
       "      <th>splice</th>\n",
       "      <th>beam_list</th>\n",
       "      <th>column_list</th>\n",
       "      <th>pz_list</th>\n",
       "      <th>colSplice</th>\n",
       "      <th>storyHeight</th>\n",
       "      <th>bayWidth</th>\n",
       "      <th>Tcond</th>\n",
       "      <th>HC_maf</th>\n",
       "      <th>...</th>\n",
       "      <th>meanM_225</th>\n",
       "      <th>meanM_475</th>\n",
       "      <th>meanM_975</th>\n",
       "      <th>meanM_2475</th>\n",
       "      <th>meanR_72</th>\n",
       "      <th>meanR_140</th>\n",
       "      <th>meanR_225</th>\n",
       "      <th>meanR_475</th>\n",
       "      <th>meanR_975</th>\n",
       "      <th>meanR_2475</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[[1,1,1,1,1,0,1,1,1],[1,1,1,1,1,1,1,1,1],[1,1,...</td>\n",
       "      <td>[[1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1],[...</td>\n",
       "      <td>[[0,0,0,0,0,0,0,0,0,0],[1,1,1,1,1,1,1,1,1,1],[...</td>\n",
       "      <td>[[0,0,0,0,0,0,0,0,0,0],[1,1,1,1,1,1,1,1,1,1],[...</td>\n",
       "      <td>[168,156,156,156,156,156,156,120,120,120,120,1...</td>\n",
       "      <td>[360,360,360,360,360,360,360,360,360]</td>\n",
       "      <td>2.21421</td>\n",
       "      <td>[0.27213836,0.16862609,0.10778856,0.074009987,...</td>\n",
       "      <td>...</td>\n",
       "      <td>7.52</td>\n",
       "      <td>7.6275</td>\n",
       "      <td>7.6975</td>\n",
       "      <td>7.7675</td>\n",
       "      <td>38.0215</td>\n",
       "      <td>28.6287</td>\n",
       "      <td>24.0991</td>\n",
       "      <td>19.7812</td>\n",
       "      <td>17.4821</td>\n",
       "      <td>15.9414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   OBJECTID  splice                                          beam_list  \\\n",
       "0         1       1  [[1,1,1,1,1,0,1,1,1],[1,1,1,1,1,1,1,1,1],[1,1,...   \n",
       "\n",
       "                                         column_list  \\\n",
       "0  [[1,1,1,1,1,1,1,1,1,1],[1,1,1,1,1,1,1,1,1,1],[...   \n",
       "\n",
       "                                             pz_list  \\\n",
       "0  [[0,0,0,0,0,0,0,0,0,0],[1,1,1,1,1,1,1,1,1,1],[...   \n",
       "\n",
       "                                           colSplice  \\\n",
       "0  [[0,0,0,0,0,0,0,0,0,0],[1,1,1,1,1,1,1,1,1,1],[...   \n",
       "\n",
       "                                         storyHeight  \\\n",
       "0  [168,156,156,156,156,156,156,120,120,120,120,1...   \n",
       "\n",
       "                                bayWidth    Tcond  \\\n",
       "0  [360,360,360,360,360,360,360,360,360]  2.21421   \n",
       "\n",
       "                                              HC_maf  ... meanM_225  \\\n",
       "0  [0.27213836,0.16862609,0.10778856,0.074009987,...  ...      7.52   \n",
       "\n",
       "   meanM_475  meanM_975  meanM_2475  meanR_72  meanR_140  meanR_225  \\\n",
       "0     7.6275     7.6975      7.7675   38.0215    28.6287    24.0991   \n",
       "\n",
       "   meanR_475  meanR_975  meanR_2475  \n",
       "0    19.7812    17.4821     15.9414  \n",
       "\n",
       "[1 rows x 30 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COLLECT EDP RESULTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 1: Parse each stripe per case per stripe for paralell data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder Example_Bldg1 already exists\n"
     ]
    }
   ],
   "source": [
    "# Initialize data (for EDP table) -> these lists include cases x stripe\n",
    "model_name_all          = []\n",
    "stripe_folder_all       = []\n",
    "save_results_folder_all = []\n",
    "msa_folder_all          = []\n",
    "beam_list_all           = []\n",
    "\n",
    "splice_all              = []\n",
    "splice_list_all         = []\n",
    "column_list_all         = []\n",
    "pz_list_all             = []\n",
    "\n",
    "for case_i in range(n_cases):\n",
    "    # Building model\n",
    "    splice       = inputs.splice[case_i]\n",
    "\n",
    "    # Building attributes\n",
    "    beam_list   = np.array(eval(inputs.beam_list[case_i]))\n",
    "    column_list = np.array(eval(inputs.column_list[case_i]))\n",
    "    if splice:\n",
    "        splice_list   = np.array(eval(inputs.colSplice[case_i]))\n",
    "    \n",
    "    # Save folderpath\n",
    "    save_results_folder = save_folder\n",
    "    try:\n",
    "        os.mkdir(save_results_folder)\n",
    "    except:\n",
    "        print('Folder '+bldg_name+' already exists')\n",
    "    \n",
    "    # Folder names with results on specified directions\n",
    "    \n",
    "    # MSA results\n",
    "    msa_folder = posixpath.join(results_folder, 'AnalysisResult', 'MSA')\n",
    "    stripe_folders = os.listdir(msa_folder)\n",
    "    n_stripes = len(stripe_folders)\n",
    "\n",
    "    # Save data for each case to run in parallel (ONLY COLLECT IF RESULTS DON'T EXIST)\n",
    "    for stripe in stripe_folders:\n",
    "        \n",
    "        results_filename = posixpath.join(save_results_folder, bldg_name + '_' + stripe + '.csv')\n",
    "        \n",
    "        if not os.path.isfile(results_filename):\n",
    "            model_name_all.append(bldg_name)\n",
    "            stripe_folder_all.append(stripe)\n",
    "            save_results_folder_all.append(save_results_folder)\n",
    "            msa_folder_all.append(msa_folder)\n",
    "            beam_list_all.append(beam_list)\n",
    "            splice_all.append(splice)            \n",
    "            if splice:\n",
    "                splice_list_all.append(splice_list)\n",
    "                column_list_all.append(column_list)\n",
    "            else:          \n",
    "                splice_list_all.append(np.zeros(column_list.shape))\n",
    "                column_list_all.append(np.zeros(column_list.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2_Collected_NLRHA_results', '2_Collected_NLRHA_results']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_results_folder_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL PARALLEL JOBS TO COLLECT = 2\n",
      "TOTAL CORES = 8\n"
     ]
    }
   ],
   "source": [
    "n_stripsBycases = len(model_name_all)\n",
    "print('TOTAL PARALLEL JOBS TO COLLECT = ' + str(n_stripsBycases))\n",
    "print('TOTAL CORES = ' + str(mp.cpu_count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 2: Collect RHA results in parallel (one job per stripe in all selected cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pool = mp.Pool(mp.cpu_count())\n",
    "part_collector = partial(collect_singleDir_response, model_name_all, stripe_folder_all, save_results_folder_all, msa_folder_all,\n",
    "                           beam_list_all, column_list_all, fracElement, splice_all, splice_list_all,\n",
    "                           minrdrift, splice_frac_strain, drift_out, rdrift_out)\n",
    "pool.map(part_collector, [i for i in range(n_stripsBycases)])\n",
    "pool.close()\n",
    "pool.join()\n",
    "print('EDPs COLLECTED AND FILES CREATED')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### STEP 2: Check number of non-convergence runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp_list = os.listdir(posixpath.join(results_folder, 'AnalysisResult', 'MSA'))\n",
    "convergenceIsCol = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for case_i, model_name in enumerate(model_name_all):\n",
    "    # Initilize dict to store stripe results for this case\n",
    "    results_build = {}\n",
    "    \n",
    "    # Identify building name\n",
    "    print(model_name)\n",
    "    bldg_name = model_name.split('_')[0]\n",
    "    \n",
    "#     # Identify return periods\n",
    "#     dirCase      = inputs.dir[case_i]\n",
    "#     msa_folder = posixpath.join(results_folder, model_name, 'AnalysisResult', 'MSA', dirCase)\n",
    "#     rp_list = os.listdir(msa_folder)\n",
    "#     n_stripes = len(rp_list)\n",
    "    \n",
    "    # load results per stripe\n",
    "    for rp in rp_list:        \n",
    "        try:\n",
    "            filename = 'EDP_' + model_name + '_' + rp +'.csv'\n",
    "            results_path = posixpath.join(save_folder, filename)\n",
    "            results_build[rp] = pd.read_csv(results_path)\n",
    "        except:\n",
    "            print('Skip rp=' + rp)\n",
    "    results[model_name] = results_build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pc   = {}\n",
    "Pincon   = {}\n",
    "n_collapses  = {}\n",
    "n_gms  = {}\n",
    "n_inconv = {}\n",
    "\n",
    "for case_i, model_name in enumerate(model_name_all):   \n",
    "    \n",
    "    # Identify building name\n",
    "    print(model_name)\n",
    "    bldg_name = model_name.split('_')[0]\n",
    "    \n",
    "#     # Identify return periods\n",
    "#     dirCase      = inputs.dir[case_i]\n",
    "#     msa_folder = posixpath.join(results_folder, model_name, 'AnalysisResult', 'MSA', dirCase)\n",
    "#     rp_list = os.listdir(msa_folder)\n",
    "#     n_stripes = len(rp_list)\n",
    "    \n",
    "    # compute collapse fraction considering both direction\n",
    "    pc  = np.zeros(len(rp_list))    \n",
    "    n_collapses_rp  = np.zeros(len(rp_list))    \n",
    "    n_gms_rp  = np.zeros(len(rp_list)) \n",
    "    n_inconv_rp = np.zeros(len(rp_list)) \n",
    "    pincon  = []\n",
    "    \n",
    "    for rp in list(results[model_name].keys()):\n",
    "        rp_i = np.argwhere(np.array(rp_list) == rp)[0][0]\n",
    "                       \n",
    "        # Get RSN for each ground motion\n",
    "        record_names = np.array(results[model_name][rp]['Unnamed: 0'])\n",
    "#         record_names\n",
    "        rsn = []\n",
    "        for i in range(len(record_names)):\n",
    "            rsn.append(record_names[i].split('_')[0])\n",
    "        rsn = np.array(rsn)    \n",
    "        \n",
    "        # Get the end criteria for corresponding components\n",
    "        endCriteria = np.array(results[model_name][rp]['EndCriteria'])\n",
    "        \n",
    "        # Compute collapse fractions\n",
    "        if convergenceIsCol:\n",
    "            # Considering non-convergence as collapse\n",
    "            nonCollapse = endCriteria == 'nonCollapse'\n",
    "            \n",
    "            pc[rp_i] = (len(nonCollapse) - sum(nonCollapse))/len(nonCollapse)\n",
    "            n_collapses_rp[rp_i] = len(nonCollapse) - sum(nonCollapse)\n",
    "            n_gms_rp[rp_i] = len(nonCollapse)\n",
    "            \n",
    "            inconv_num = 0\n",
    "            for i in range(len(endCriteria)):\n",
    "                if endCriteria[i] == 'Inconvergence':\n",
    "                    inconv_num += 1\n",
    "            n_inconv_rp[rp_i] = inconv_num\n",
    "            \n",
    "            inconvergence = endCriteria == 'Inconvergence'\n",
    "            pincon.append(sum(inconvergence)/len(inconvergence))                      \n",
    "            \n",
    "        else:\n",
    "            col_num    = 0         \n",
    "            total_num  = 0   \n",
    "            inconv_num = 0\n",
    "            for i in range(len(endCriteria)):\n",
    "                if endCriteria[i] != 'Inconvergence':\n",
    "                    total_num += 1\n",
    "                    if endCriteria[i] == 'MaxDrift':\n",
    "                        col_num += 1\n",
    "                else:\n",
    "                    inconv_num += 1\n",
    "            \n",
    "            pc[rp_i] = col_num/total_num\n",
    "            n_collapses_rp[rp_i] = col_num\n",
    "            n_gms_rp[rp_i] = total_num\n",
    "            n_inconv_rp[rp_i] = inconv_num\n",
    "    \n",
    "    Pc[model_name]      = pc\n",
    "    n_collapses[model_name] = n_collapses_rp\n",
    "    n_gms[model_name]   = n_gms_rp\n",
    "    n_inconv[model_name] = n_inconv_rp\n",
    "    \n",
    "    if convergenceIsCol:\n",
    "        Pincon[model_name] = pincon\n",
    "        \n",
    "Pincon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COLLECT ENDSTATE PER CASE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 1: Parse each stripe per case for paralell data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data (inputs for endState results)\n",
    "model_name_all          = []\n",
    "stripe_folders_all      = []\n",
    "save_results_folder_all = []\n",
    "msa_folder_all          = []\n",
    "beam_list_all         = []\n",
    "column_list_all       = []\n",
    "pz_list_all           = []\n",
    "splice_all              = []\n",
    "colSplice_all         = []\n",
    "\n",
    "for case_i in range(n_cases):\n",
    "    # Building model\n",
    "    splice       = inputs.splice[case_i]\n",
    "\n",
    "    # Building attributes\n",
    "    beam_list   = np.array(eval(inputs.beam_list[case_i]))\n",
    "    column_list = np.array(eval(inputs.column_list[case_i]))\n",
    "    pz_list = np.array(eval(inputs.pz_list[case_i]))\n",
    "    if splice:\n",
    "        colSplice = np.array(eval(inputs.colSplice[case_i]))\n",
    "\n",
    "    # Save folderpath (does not need to create it since previous block already did it)\n",
    "    save_results_folder = save_folder\n",
    "    \n",
    "    # Folder names with results on both directions\n",
    "\n",
    "    # MSA results\n",
    "    msa_folder = posixpath.join(results_folder, 'AnalysisResult', 'MSA')\n",
    "    stripe_folders = os.listdir(msa_folder)\n",
    "    n_stripes = len(stripe_folders)\n",
    "\n",
    "    # Save data for each case to run in parallel\n",
    "    model_name_all.append(model_name)\n",
    "    stripe_folders_all.append(stripe_folders)\n",
    "    save_results_folder_all.append(save_results_folder)\n",
    "    msa_folder_all.append(msa_folder)\n",
    "    beam_list_all.append(beam_list)\n",
    "    column_list_all.append(column_list)\n",
    "    pz_list_all.append(pz_list)\n",
    "    splice_all.append(splice)            \n",
    "    if splice:\n",
    "        colSplice_all.append(colSplice)     \n",
    "    else:            \n",
    "        colSplice_all.append(np.zeros(column_list.shape))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_endState_singleDir_response(model_name_all, save_results_folder_all, stripe_folders_all, msa_folders_all, beam_list_all,\n",
    "                               column_list_all, pz_list_all, splice_all, colSplice_all, case_i):\n",
    "    # INPUTS\n",
    "    # All the inputs include information per case (different from the EDP collector that breaks each case into independent jobs per stripe\n",
    "    #    model_name_all           = list of str with the case name to collect results from\n",
    "    #    save_results_folder_all  = list of str with path to save results\n",
    "    #    stripe_folders_all       = list of list with the folder name of each stripe\n",
    "    #    msa_folders_all          = list eith the path_to_results\n",
    "    #    beam_list_all          = list of 2D np.array indicating which beams exist in the X frame\n",
    "    #    column_list_all        = list of 2D np.array indicating which columns exist in the X frame\n",
    "    #    pz_list_all            = list of 2D np.array indicating which pz exist in the X frame\n",
    "    #    splice_all               = list of boolean if splice are considered or ignored\n",
    "    #    colSplice_all          = list of 2D np.array indicating which stories have a splice in the X frame\n",
    "    #\n",
    "\n",
    "    # Parse case to execute\n",
    "    model_name    = model_name_all[case_i]\n",
    "    save_results_folder = save_results_folder_all[case_i]\n",
    "    stripe_folders= stripe_folders_all[case_i]\n",
    "    msa_folders   = msa_folders_all[case_i]\n",
    "    beam_list     = beam_list_all[case_i]\n",
    "    column_list   = column_list_all[case_i]\n",
    "    pz_list       = pz_list_all[case_i]\n",
    "    splice        = splice_all[case_i]\n",
    "    if splice == 1:\n",
    "        colSplice = colSplice_all[case_i]\n",
    "\n",
    "    n_stripes = len(stripe_folders)\n",
    "    # Collect end state for every gm in every return period for BOTH DIRECTIONS\n",
    "\n",
    "    print('------- ' + model_name + ' -------')\n",
    "\n",
    "    # Select frame geometry matrices      \n",
    "    results_filename = posixpath.join(save_results_folder, model_name + '.h5')\n",
    "\n",
    "    # # count panel zones\n",
    "    n_stories, n_pier = column_list.shape\n",
    "    num_pz = 0\n",
    "    for i_story in range(n_stories):\n",
    "        for i_pier in range(n_pier):\n",
    "            if ((column_list[min(i_story + 1, n_stories-1), i_pier] == 1) or\n",
    "                    ((column_list[i_story, i_pier] == 1 )) and\n",
    "                    (beam_list[i_story, i_pier - 1])):\n",
    "                existPZ = True\n",
    "            elif ((column_list[min(i_story + 1, n_stories-1), i_pier] == 1) or\n",
    "                    ((column_list[i_story, i_pier] == 1 ) and\n",
    "                     (beam_list[i_story, min(i_pier, n_pier -2)]))):\n",
    "                existPZ = True\n",
    "            else:\n",
    "                existPZ = False\n",
    "\n",
    "            if existPZ:\n",
    "                num_pz += 1\n",
    "    print('num_pz='+str(num_pz))\n",
    "\n",
    "    # Removes existing file\n",
    "    if os.path.isfile(results_filename):\n",
    "        os.remove(results_filename)\n",
    "        print(results_filename + ' already exists, so deleted it')\n",
    "\n",
    "    # if True (Collects only data for those building without data)\n",
    "    if not os.path.isfile(results_filename):\n",
    "\n",
    "        # Collect results and store in HDF file\n",
    "        with h5py.File(results_filename, 'w') as hf:\n",
    "            # prepare data groups per return period\n",
    "            for group in stripe_folders:\n",
    "                _ = hf.create_group('/' + group)\n",
    "\n",
    "            # collect bldg response for each gm in each stripe\n",
    "            for i in range(n_stripes):\n",
    "                stripe_folder_path = posixpath.join(msa_folders, stripe_folders[i])\n",
    "\n",
    "                print('RP = ' + str(stripe_folders[i]) + 'years')\n",
    "                # print(stripe_folder_path)\n",
    "\n",
    "                gm_ids = os.listdir(stripe_folder_path)\n",
    "                for j in range(len(gm_ids)):\n",
    "                    # print(gm_ids[j])\n",
    "\n",
    "                    # collect results for this gm\n",
    "                    results_gm = posixpath.join(stripe_folder_path, gm_ids[j])\n",
    "\n",
    "                    # check if acc results available (gm finished?)\n",
    "                    pfa_gm = get_EDPstory_response(results_gm, n_stories, 'acc_env')\n",
    "                    if type(pfa_gm) == int:  # did not finish RHA, so skip the ground motion\n",
    "                        print('Did not finish GM' + str(gm_ids[j]))\n",
    "                    else:\n",
    "                        #    Panel zones\n",
    "                        pz_response = get_pz_response(results_gm, pz_list, ['all_disp', 'pz_rot'])\n",
    "                        #    beams and columns\n",
    "                        column_response = get_column_response(results_gm, column_list, ['hinge_bot','hinge_top'])\n",
    "                        beam_plas_rot = get_beam_response(results_gm, beam_list, ['hinge_left', 'hinge_right'])\n",
    "                        frac_simulated  = get_beam_response(results_gm, beam_list, ['frac_LB','frac_LT','frac_RB','frac_RT'])\n",
    "                        #    Splices\n",
    "                        if splice == 1:\n",
    "                            splice_response = get_splice_response(results_gm, colSplice, column_list, ['ss_splice'],\n",
    "                                              res_type='Max', def_desired='strain')\n",
    "                            splice_frac = splice_response['ss_splice'] > 2*60/29000\n",
    "\n",
    "                        # create gm group\n",
    "                        rp_group = hf['/' + stripe_folders[i]]\n",
    "                        gm_record_group = rp_group.create_group(gm_ids[j])\n",
    "\n",
    "                        # Save in h5 file's building_group\n",
    "                        key = 'all_disp'\n",
    "                        _ = gm_record_group.create_dataset(key, data=pz_response[key])\n",
    "                        key = 'pz_rot'\n",
    "                        _ = gm_record_group.create_dataset(key, data=pz_response[key])\n",
    "                        key = 'hinge_bot'\n",
    "                        _ = gm_record_group.create_dataset(key, data=column_response[key])\n",
    "                        key = 'hinge_top'\n",
    "                        _ = gm_record_group.create_dataset(key, data=column_response[key])\n",
    "                        key = 'hinge_left'\n",
    "                        _ = gm_record_group.create_dataset(key, data=beam_plas_rot[key])\n",
    "                        key = 'hinge_right'\n",
    "                        _ = gm_record_group.create_dataset(key, data=beam_plas_rot[key])\n",
    "                        key = 'frac_LB'\n",
    "                        _ = gm_record_group.create_dataset(key, data=frac_simulated[key])\n",
    "                        key = 'frac_LT'\n",
    "                        _ = gm_record_group.create_dataset(key, data=frac_simulated[key])\n",
    "                        key = 'frac_RB'\n",
    "                        _ = gm_record_group.create_dataset(key, data=frac_simulated[key])\n",
    "                        key = 'frac_RT'\n",
    "                        _ = gm_record_group.create_dataset(key, data=frac_simulated[key])\n",
    "                        if splice == 1:\n",
    "                            key = 'ss_splice'\n",
    "                            _ = gm_record_group.create_dataset(key, data=splice_response[key])\n",
    "                            key = 'splice_frac'\n",
    "                            _ = gm_record_group.create_dataset(key, data=splice_frac)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3: Collect end state of each building in parallel (one job per case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_endState_singleDir_response(model_name_all, save_results_folder_all, stripe_folders_all, msa_folder_all, beam_list_all,\n",
    "                            column_list_all, pz_list_all, splice_all, \n",
    "                               colSplice_all, case_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
